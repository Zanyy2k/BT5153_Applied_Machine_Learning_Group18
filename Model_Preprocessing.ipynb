{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1ftEXHCuFh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# refine preprocessing\n",
        "import re\n",
        "\n",
        "# Replace all numeric with 'n'\n",
        "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
        "\n",
        "COMMENT_COL = 'comment_text'\n",
        "ID_COL = 'id'\n",
        "\n",
        "# redundancy words and their right formats\n",
        "redundancy_rightFormat = {\n",
        "    'ckckck': 'cock',\n",
        "    'fuckfuck': 'fuck',\n",
        "    'lolol': 'lol',\n",
        "    'lollol': 'lol',\n",
        "    'pussyfuck':'fuck',\n",
        "    'gaygay': 'gay',\n",
        "    'haha': 'ha',\n",
        "    'sucksuck': 'suck'}\n",
        "\n",
        "redundancy = set(redundancy_rightFormat.keys())\n",
        "\n",
        "# all the words below are included in glove dictionary\n",
        "# combine these toxic indicators with 'CommProcess.revise_triple_and_more_letters'\n",
        "toxic_indicator_words = [\n",
        "    'fuck', 'fucking', 'fucked', 'fuckin', 'fucka', 'fucker', 'fucks', 'fuckers',\n",
        "    'fck', 'fcking', 'fcked', 'fckin', 'fcker', 'fcks',\n",
        "    'fuk', 'fuking', 'fuked', 'fukin', 'fuker', 'fuks', 'fukers',\n",
        "    'fk', 'fking', 'fked', 'fkin', 'fker', 'fks',\n",
        "    'shit', 'shitty', 'shite',\n",
        "    'stupid', 'stupids',\n",
        "    'idiot', 'idiots',\n",
        "    'suck', 'sucker', 'sucks', 'sucka', 'sucked', 'sucking',\n",
        "    'ass', 'asses', 'asshole', 'assholes', 'ashole', 'asholes',\n",
        "    'gay', 'gays',\n",
        "    'niga', 'nigga', 'nigar', 'niggar', 'niger', 'nigger',\n",
        "    'monster', 'monsters',\n",
        "    'loser', 'losers',\n",
        "    'nazi', 'nazis',\n",
        "    'cock', 'cocks', 'cocker', 'cockers',\n",
        "    'faggot', 'faggy',\n",
        "]\n",
        "toxic_indicator_words_sets = set(toxic_indicator_words)\n",
        "\n",
        "def _get_toxicIndicator_transformers():\n",
        "    toxicIndicator_transformers = dict()\n",
        "    for word in toxic_indicator_words:\n",
        "        tmp_1 = []\n",
        "        for c in word:\n",
        "            if len(tmp_1) > 0:\n",
        "                tmp_2 = []\n",
        "                for pre in tmp_1:\n",
        "                    tmp_2.append(pre + c)\n",
        "                    tmp_2.append(pre + c + c)\n",
        "                tmp_1 = tmp_2\n",
        "            else:\n",
        "                tmp_1.append(c)\n",
        "                tmp_1.append(c + c)\n",
        "        toxicIndicator_transformers[word] = tmp_1\n",
        "    return toxicIndicator_transformers\n",
        "\n",
        "toxicIndicator_transformers = _get_toxicIndicator_transformers()\n",
        "\n",
        "deny_origin = {\n",
        "    \"you're\": ['you', 'are'],\n",
        "    \"i'm\": ['i', 'am'],\n",
        "    \"he's\": ['he', 'is'],\n",
        "    \"she's\": ['she', 'is'],\n",
        "    \"it's\": ['it', 'is'],\n",
        "    \"they're\": ['they', 'are'],\n",
        "    \"can't\": ['can', 'not'],\n",
        "    \"couldn't\": ['could', 'not'],\n",
        "    \"don't\": ['do', 'not'],\n",
        "    \"don;t\": ['do', 'not'],\n",
        "    \"didn't\": ['did', 'not'],\n",
        "    \"doesn't\": ['does', 'not'],\n",
        "    \"isn't\": ['is', 'not'],\n",
        "    \"wasn't\": ['was', 'not'],\n",
        "    \"aren't\": ['are', 'not'],\n",
        "    \"weren't\": ['were', 'not'],\n",
        "    \"won't\": ['will', 'not'],\n",
        "    \"wouldn't\": ['would', 'not'],\n",
        "    \"hasn't\": ['has', 'not'],\n",
        "    \"haven't\": ['have', 'not'],\n",
        "    \"what's\": ['what', 'is'],\n",
        "    \"that's\": ['that', 'is'],\n",
        "}\n",
        "denies = set(deny_origin.keys())\n",
        "\n",
        "\n",
        "class CommProcess(object):\n",
        "    @staticmethod\n",
        "    def clean_text(t):\n",
        "        t = re.sub(r\"[^A-Za-z0-9,!?*.;’´'\\/]\", \" \", t)\n",
        "        t = replace_numbers.sub(\" \", t)\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\",\", \" \", t)\n",
        "        t = re.sub(r\"’\", \"'\", t)\n",
        "        t = re.sub(r\"´\", \"'\", t)\n",
        "        t = re.sub(r\"\\.\", \" \", t)\n",
        "        t = re.sub(r\"!\", \" ! \", t)\n",
        "        t = re.sub(r\"\\?\", \" ? \", t)\n",
        "        t = re.sub(r\"\\/\", \" \", t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_deny(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if word in denies:\n",
        "                ret.append(deny_origin[word][0])\n",
        "                ret.append(deny_origin[word][1])\n",
        "            else:\n",
        "                ret.append(word)\n",
        "        ret = ' '.join(ret)\n",
        "        ret = re.sub(\"'\", \" \", ret)\n",
        "        ret = re.sub(r\";\", \" \", ret)\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_star(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if ('*' in word) and (re.sub('\\*', '', word) in toxic_indicator_words_sets):\n",
        "                word = re.sub('\\*', '', word)\n",
        "            ret.append(word)\n",
        "        ret = re.sub('\\*', ' ', ' '.join(ret))\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_triple_and_more_letters(t):\n",
        "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "            reg = letter + \"{2,}\"\n",
        "            t = re.sub(reg, letter + letter, t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_redundancy_words(t):\n",
        "        ret = []\n",
        "        for word in t.split(' '):\n",
        "            for redu in redundancy:\n",
        "                if redu in word:\n",
        "                    word = redundancy_rightFormat[redu]\n",
        "                    break\n",
        "            ret.append(word)\n",
        "        return ' '.join(ret)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_na(t):\n",
        "        if t.strip() == '':\n",
        "            return 'NA'\n",
        "        return t\n",
        "\n",
        "\n",
        "def execute_comm_process(df):\n",
        "    df = copy.deepcopy(df)\n",
        "    comm_process_pipeline = [\n",
        "        CommProcess.clean_text,\n",
        "        CommProcess.revise_deny,\n",
        "        CommProcess.revise_star,\n",
        "        CommProcess.revise_triple_and_more_letters,\n",
        "        CommProcess.revise_redundancy_words,\n",
        "        CommProcess.fill_na,\n",
        "    ]\n",
        "    for cp in comm_process_pipeline:\n",
        "        df[COMMENT_COL] = df[COMMENT_COL].apply(cp)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dlCMTXouMQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process training data\n",
        "print('Comm processing train data')\n",
        "df_train_clean = execute_comm_process(df_train)\n",
        "df_train_clean.to_csv('train_train_final_processed.csv', index=False)\n",
        "# Process val data\n",
        "print('Comm processing val data')\n",
        "df_val_clean = execute_comm_process(df_val)\n",
        "df_val_clean.to_csv('train_val_final_processed.csv', index=False)\n",
        "# Process test data\n",
        "print('Comm processing test data')\n",
        "df_test_clean = execute_comm_process(df_test)\n",
        "df_test_clean.to_csv('test_final_processed.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}